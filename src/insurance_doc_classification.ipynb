{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Document Classification with RoBERTa\n",
    "\n",
    "This notebook demonstrates how to build an intelligent insurance document classifier using the DistilRoBERTa base model. The classifier will be able to categorize insurance documents into three classes:\n",
    "\n",
    "1. Policy documents \n",
    "2. Claims\n",
    "3. Support queries\n",
    "\n",
    "We chose DistilRoBERTa because:\n",
    "- It's a lighter, distilled version of RoBERTa that maintains strong performance\n",
    "- RoBERTa is particularly good at document classification tasks due to its robust pretraining\n",
    "- The model size (~82M parameters) provides a good balance between accuracy and computational efficiency\n",
    "- The distilled architecture allows for faster inference in production\n",
    "\n",
    "The notebook covers:\n",
    "- Data preparation and preprocessing\n",
    "- Model fine-tuning\n",
    "- Evaluation\n",
    "- Building an inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_data import load_and_prepare_data, create_datasets\n",
    "from model_training import ModelTrainer\n",
    "from insurance_agent import InsuranceAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We first need to prepare out synthetic data for fine tuning our model. We'd want many more examples than 100, but this is just a demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: ../data/synthetic_insurance_data.json\n",
      "\n",
      "Dataset split complete:\n",
      "Training examples: 86\n",
      "Testing examples: 22\n"
     ]
    }
   ],
   "source": [
    "data_splits = load_and_prepare_data(file_path=\"../data/synthetic_insurance_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing tokenizer: distilroberta-base\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "tokenizer, train_dataset, test_dataset = create_datasets(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example from training dataset:\n",
      "Input IDs shape: torch.Size([69])\n",
      "Attention mask shape: torch.Size([69])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example from our training dataset\n",
    "print(\"\\nExample from training dataset:\")\n",
    "example_idx = 0\n",
    "example = train_dataset[example_idx]\n",
    "\n",
    "print(\"Input IDs shape:\", example['input_ids'].shape)\n",
    "print(\"Attention mask shape:\", example['attention_mask'].shape)\n",
    "print(\"Label:\", example['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that we can use to fine tune our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning RoBERTa\n",
    "\n",
    "We'll use the `AutoModelForSequenceClassification` class to fine tune our RoBERTa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing distilroberta-base for fine-tuning...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.100400</td>\n",
       "      <td>0.709337</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.910279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "\n",
      "Evaluation Results:\n",
      "eval_loss: 0.0170\n",
      "eval_accuracy: 1.0000\n",
      "eval_f1: 1.0000\n",
      "eval_runtime: 2.5593\n",
      "eval_samples_per_second: 8.5960\n",
      "eval_steps_per_second: 1.1720\n",
      "epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer()\n",
    "\n",
    "trained_model, eval_results = trainer.train(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model and tokenizer...\n",
      "\n",
      "Model and tokenizer saved to './insurance-model'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSaving model and tokenizer...\")\n",
    "trained_model.save_model(\"../models/insurance-model\")\n",
    "tokenizer.save_pretrained(\"../models/insurance-model\")\n",
    "\n",
    "print(\"\\nModel and tokenizer saved to './insurance-model'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our model is trained for demo purposes. It overfit on the training data due to such a small dataset. In practice, we'd want a larger data set and would focus on improving the training parameters to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Approach\n",
    "\n",
    "Now that we have a fine-tuned model, we can use it to classify documents. The `InsuranceAgent` class is a simple agent that uses the fine-tuned model to classify documents. Downstream operations are in place for more complex tasks, but are not implemented in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Insurance Agent...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "agent = InsuranceAgent(model_path=\"../models/insurance-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents to process\n",
    "test_documents = [\n",
    "    {\n",
    "        \"type\": \"Policy Document\",\n",
    "        \"text\": \"This cyber insurance policy provides coverage for data breaches and ransomware attacks. Policy limits: $1M per occurrence.\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"Claim Submission\",\n",
    "        \"text\": \"Filing a claim for ransomware attack that occurred on Aug 15, 2024. Systems were encrypted and business was interrupted for 48 hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"Support Query\",\n",
    "        \"text\": \"Need clarification on cyber coverage limits and exclusions for cloud service provider outages.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Policy Document...\n",
      "\n",
      "Results:\n",
      "Document Type: policy\n",
      "Confidence: 99.06%\n",
      "Response: {'action': 'policy_analysis', 'message': 'Policy analysis would extract key information, coverage details, and generate recommendations.', 'sample_data': {'policy_type': 'Example Policy Type', 'coverage_amount': '$500,000', 'effective_date': '2024-01-01'}}\n",
      "Next Steps: ['Example Step 1 for policy', 'Example Step 2 for policy', 'Contact support if needed']\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing Claim Submission...\n",
      "\n",
      "Results:\n",
      "Document Type: claim\n",
      "Confidence: 99.22%\n",
      "Response: {'action': 'claim_processing', 'message': 'Claim processing would validate details, assess urgency, and determine required documentation.', 'sample_data': {'claim_id': 'CLM123456', 'status': 'Under Review', 'priority': 'Medium'}}\n",
      "Next Steps: ['Example Step 1 for claim', 'Example Step 2 for claim', 'Contact support if needed']\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing Support Query...\n",
      "\n",
      "Results:\n",
      "Document Type: support\n",
      "Confidence: 98.25%\n",
      "Response: {'action': 'support_response', 'message': 'Support system would categorize query, generate response, and identify relevant resources.', 'sample_data': {'query_type': 'Coverage Question', 'response_time': '24 hours', 'category': 'Policy Inquiry'}}\n",
      "Next Steps: ['Example Step 1 for support', 'Example Step 2 for support', 'Contact support if needed']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process each document and show results\n",
    "for doc in test_documents:\n",
    "    print(f\"\\nProcessing {doc['type']}...\")\n",
    "    result = agent.process_document(doc['text'])\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Document Type: {result['document_type']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"Next Steps: {result['next_steps']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see our model properly classifies the documents and provides hypothetical next steps. This demo is simple, but shows how we can use a fine-tuned model to handle a wide range of tasks.\n",
    "\n",
    "Building specialized agents for each document type (policy, claims, support) would allow us to incorporate domain-specific logic and optimizations. For example, a dedicated claims agent could have specialized NER models for extracting incident details, while a policy agent could focus on coverage analysis and risk assessment.\n",
    "\n",
    "For even more sophisticated processing, we can leverage different transformer architectures based on the specific needs of each agent:\n",
    "T5 models excel at structured question answering tasks, making them ideal for claims processing agents that need to extract specific details from incident reports. For instance, the claims agent could use T5 to systematically query the document: \"When did the incident occur?\", \"What type of damage occurred?\", \"Were there any witnesses?\" T5's encoder-decoder architecture is particularly well-suited for these focused extraction tasks where we need precise, factual answers.\n",
    "\n",
    "GPT models, with their autoregressive architecture, are better suited for more open-ended generation tasks. A policy analysis agent could leverage GPT to generate comprehensive coverage summaries, identify potential coverage gaps, or explain complex policy terms in plain language. The model's strength in maintaining context and generating coherent, contextually relevant text makes it valuable for tasks requiring more nuanced understanding and explanation.\n",
    "\n",
    "We could also implement hybrid approaches where different models handle different aspects of the processing pipeline. For example, a support ticket agent might use T5 to extract key issue details and categorize the request, then use GPT to generate appropriate response templates or suggest resolution steps. This combination allows us to leverage the strengths of each architecture - T5's precision in information extraction and GPT's fluency in generation.\n",
    "\n",
    "The key is matching the model architecture to the specific requirements of each agent's tasks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
